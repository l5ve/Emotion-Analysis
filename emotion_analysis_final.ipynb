{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ju907\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ju907\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "# Set Pandas to disply all rows of dataframes\n",
    "pd.set_option('display.max_rows',500)\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# For implementation of parallelization\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import Process, Queue, current_process\n",
    "import parmap\n",
    "import tqdm\n",
    "from multiprocessing import Manager\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "import s_a_util\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(num_cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Make string(lyric) to list(words of lyric) after some processes.\n",
    "    \n",
    "    Args: \n",
    "        text (str): raw lyric of a song\n",
    "    Returns:\n",
    "        tokens (list): words list that make up the lyric.\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    # tokenize into words\n",
    "    tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # lower capitalization\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    \n",
    "    # lemmatization\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "    tokens = [lmtzr.lemmatize(word, 'v') for word in tokens]\n",
    "  \n",
    "          \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyric_dic(t_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Make word-list to word-dictionary.\n",
    "    \n",
    "    Args:\n",
    "        t_list(list) : list of words.\n",
    "    Returns:\n",
    "        dic(dic) : dictionary of words\n",
    "    \n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    for i in t_list:\n",
    "        if i in dic:\n",
    "            dic[i]+=1\n",
    "        else:\n",
    "            dic[i]=1\n",
    "            \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_song_list(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Make list of songs from song data.\n",
    "    \n",
    "    Args: \n",
    "        data(pandas.core.frame.DataFrame): dataframe which contains informataions of songs\n",
    "    Returns:\n",
    "        Song_list(list):\n",
    "    \"\"\"\n",
    "    Song_list=[]\n",
    "    for i,song in data.iterrows():\n",
    "        print(song['title'],'-',song['artist'], 'preprocessing...')\n",
    "        Song_list.append({'title': song['title'],\n",
    "                         'artist': song['artist'],\n",
    "                         'lyric': lyric_dic(preprocessing(song['lyric'])) ,\n",
    "                         'duration': song['duration'],\n",
    "                         'sentiment': {'Positive':0,'Negative':0,'Anger':0,'Anticipation':0,'Disgust':0,'Fear':0,'Joy':0,'Sadness':0,'Surprise':0,'Trust':0,'Love':0,'dominant_emo': None},\n",
    "                         \n",
    "                         })\n",
    "        \n",
    "    return Song_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_emotion_words_dic(emotion_words):\n",
    "    \n",
    "    \"\"\"\n",
    "    Make emotion_words dictionary to use fast searching in dictionary\n",
    "    \n",
    "    Args :\n",
    "        emotion_words(pandas.core.frame.DataFrame) : dataframe of NRC-Emotion-lexicon \n",
    "        \n",
    "    Returns :\n",
    "        emotion_words_dic(dic) : dictionary of NRC-Emotion-lexicon  ex) {'cry':[0,1,0,0,0,0,0,1,0,0,0],...}\n",
    "    \"\"\"\n",
    "    emotion_words_dic = {}\n",
    "    \n",
    "    for i,row in emotion_words.iterrows():\n",
    "        emotion_words_dic[row[0]]=row[1:]\n",
    "           \n",
    "            \n",
    "    return emotion_words_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunflower (Spider-Man: Into The Spider-Verse) - Post Malone & Swae Lee preprocessing...\n",
      "Without Me - Halsey preprocessing...\n",
      "Bad Guy - Billie Eilish preprocessing...\n",
      "Wow. - Post Malone preprocessing...\n",
      "Happier - Marshmello & Bastille preprocessing...\n",
      "7 Rings - Ariana Grande preprocessing...\n",
      "Talk - Khalid preprocessing...\n",
      "Sicko Mode - Travis Scott preprocessing...\n",
      "Sucker - Jonas Brothers preprocessing...\n",
      "High Hopes - Panic! At The Disco preprocessing...\n",
      "Thank U, Next - Ariana Grande preprocessing...\n",
      "Truth Hurts - Lizzo preprocessing...\n",
      "Dancing With A Stranger - Sam Smith & Normani preprocessing...\n",
      "Senorita - Shawn Mendes & Camila Cabello preprocessing...\n",
      "I Don't Care - Ed Sheeran & Justin Bieber preprocessing...\n",
      "Going Bad - Meek Mill Featuring Drake preprocessing...\n",
      "Shallow - Lady Gaga & Bradley Cooper preprocessing...\n",
      "Better - Khalid preprocessing...\n",
      "No Guidance - Chris Brown Featuring Drake preprocessing...\n",
      "Girls Like You - Maroon 5 Featuring Cardi B preprocessing...\n",
      "Sweet But Psycho - Ava Max preprocessing...\n",
      "Suge - DaBaby preprocessing...\n",
      "Middle Child - J. Cole preprocessing...\n",
      "Drip Too Hard - Lil Baby & Gunna preprocessing...\n",
      "Someone You Loved - Lewis Capaldi preprocessing...\n",
      "Ran$om - Lil Tecca preprocessing...\n",
      "If I Can't Have You - Shawn Mendes preprocessing...\n",
      "Goodbyes - Post Malone Featuring Young Thug preprocessing...\n",
      "ZEZE - Kodak Black Featuring Travis Scott & Offset preprocessing...\n",
      "Better Now - Post Malone preprocessing...\n",
      "Youngblood - 5 Seconds Of Summer preprocessing...\n",
      "Money In The Grave - Drake Featuring Rick Ross preprocessing...\n",
      "Speechless - Dan + Shay preprocessing...\n",
      "Break Up With Your Girlfriend, I'm Bored - Ariana Grande preprocessing...\n",
      "Please Me - Cardi B & Bruno Mars preprocessing...\n",
      "Money - Cardi B preprocessing...\n",
      "You Need To Calm Down - Taylor Swift preprocessing...\n",
      "Panini - Lil Nas X preprocessing...\n",
      "Look Back At It - A Boogie Wit da Hoodie preprocessing...\n",
      "A Lot - 21 Savage preprocessing...\n",
      "ME! - Taylor Swift Featuring Brendon Urie preprocessing...\n",
      "MIA - Bad Bunny Featuring Drake preprocessing...\n",
      "Pop Out - Polo G Featuring Lil Tjay preprocessing...\n",
      "Beautiful Crazy - Luke Combs preprocessing...\n",
      "Thotiana - Blueface preprocessing...\n",
      "Lucid Dreams - Juice WRLD preprocessing...\n",
      "Mo Bamba - Sheck Wes preprocessing...\n",
      "Beautiful People - Ed Sheeran Featuring Khalid preprocessing...\n",
      "Wake Up In The Sky - Gucci Mane X Bruno Mars X Kodak Black preprocessing...\n",
      "Whiskey Glasses - Morgan Wallen preprocessing...\n",
      "God's Country - Blake Shelton preprocessing...\n",
      "Be Alright - Dean Lewis preprocessing...\n",
      "Pure Water - Mustard & Migos preprocessing...\n",
      "The Git Up - Blanco Brown preprocessing...\n",
      "Taki Taki - DJ Snake Featuring Selena Gomez, Ozuna & Cardi B preprocessing...\n",
      "Close To Me - Ellie Goulding X Diplo Featuring Swae Lee preprocessing...\n",
      "Envy Me - Calboy preprocessing...\n",
      "You Say - Lauren Daigle preprocessing...\n",
      "Hey Look Ma, I Made It - Panic! At The Disco preprocessing...\n",
      "Circles - Post Malone preprocessing...\n",
      "Beer Never Broke My Heart - Luke Combs preprocessing...\n",
      "The London - Young Thug, J. Cole & Travis Scott preprocessing...\n",
      "Murder On My Mind - YNW Melly preprocessing...\n",
      "When The Party's Over - Billie Eilish preprocessing...\n",
      "Act Up - City Girls preprocessing...\n",
      "I Like It - Cardi B, Bad Bunny & J Balvin preprocessing...\n",
      "Trampoline - SHAED preprocessing...\n",
      "Leave Me Alone - Flipp Dinero preprocessing...\n",
      "Breathin - Ariana Grande preprocessing...\n",
      "Bury A Friend - Billie Eilish preprocessing...\n",
      "Close Friends - Lil Baby preprocessing...\n",
      "Baby Shark - Pinkfong preprocessing...\n",
      "My Type - Saweetie preprocessing...\n",
      "Worth It - YK Osiris preprocessing...\n",
      "Only Human - Jonas Brothers preprocessing...\n",
      "Knockin' Boots - Luke Bryan preprocessing...\n",
      "Trip - Ella Mai preprocessing...\n",
      "Rumor - Lee Brice preprocessing...\n",
      "Swervin - A Boogie Wit da Hoodie Featuring 6ix9ine preprocessing...\n",
      "How Do You Sleep? - Sam Smith preprocessing...\n",
      "Baby - Lil Baby & DaBaby preprocessing...\n",
      "Look What God Gave Her - Thomas Rhett preprocessing...\n",
      "Good As You - Kane Brown preprocessing...\n",
      "Clout - Offset Featuring Cardi B preprocessing...\n",
      "Love Lies - Khalid & Normani preprocessing...\n",
      "One Thing Right - Marshmello & Kane Brown preprocessing...\n",
      "Cash Shit - Megan Thee Stallion Featuring DaBaby preprocessing...\n",
      "Tequila - Dan + Shay preprocessing...\n",
      "Shotta Flow - NLE Choppa preprocessing...\n",
      "Hot Girl Summer - Megan Thee Stallion, Nicki Minaj & Ty Dolla $ign preprocessing...\n",
      "Talk You Out Of It - Florida Georgia Line preprocessing...\n",
      "Beautiful - Bazzi Featuring Camila Cabello preprocessing...\n",
      "Eyes On You - Chase Rice preprocessing...\n",
      "All To Myself - Dan + Shay preprocessing...\n",
      "Boyfriend - Ariana Grande & Social House preprocessing...\n",
      "Walk Me Home - P!nk preprocessing...\n",
      "Robbery - Juice WRLD preprocessing...\n",
      "Making list time : 2.3387961387634277\n",
      "analysis time(multiprocessing)for 97 songs :  8.524390935897827\n"
     ]
    }
   ],
   "source": [
    "# import collected data\n",
    "data = pd.read_excel(\"Song/2019_data.xlsx\")\n",
    "\n",
    "# import NRC-Emotion_Lixicon\n",
    "emotion_words = pd.read_csv(\"Song/NRC-Emotion-Lexicon.csv\")\n",
    "\n",
    "# make emotions_dictionary \n",
    "emo_d = making_emotion_words_dic(emotion_words)\n",
    "\n",
    "\n",
    "# drop the rows that at least one element is NAN\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "# data = data.loc[38:41]\n",
    "\n",
    "\n",
    "# making song_list which contains all informations of every song\n",
    "start = time.time()\n",
    "song_list = making_song_list(data)\n",
    "#print(song_list)\n",
    "print(\"Making list time :\", time.time() - start)\n",
    "\n",
    "\n",
    "# do emotion analysis using multiprocessing\n",
    "\n",
    "start = time.time()\n",
    "if __name__ ==  '__main__': \n",
    "    result=Manager().Queue()\n",
    "    pool = multiprocessing.Pool(processes=num_cores)\n",
    "    splited_song_list = np.array_split(song_list, num_cores)\n",
    "    spilited_song_list= [x.tolist() for x in splited_song_list]\n",
    "    pool.starmap(s_a_util.start_analysis_v2,[(song_list,emo_d,result) for song_list in splited_song_list])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "print(\"analysis time(multiprocessing)for\",len(song_list),\"songs : \", time.time() - start)  \n",
    "\n",
    "\n",
    "# match multiprocessing result to song. So, update song_list's sentiment part\n",
    "\n",
    "result.put('STOP')\n",
    "result_dic={}\n",
    "while True:\n",
    "    tmp = result.get()\n",
    "    if tmp == 'STOP':\n",
    "        break\n",
    "    else:\n",
    "        result_dic[tmp[0]]=tmp[1]\n",
    " \n",
    "        \n",
    "for i in song_list:\n",
    "    i['sentiment']=result_dic[i['title']]\n",
    "\n",
    "\n",
    "# Store result of analysis to output.csv\n",
    "\n",
    "f = open('result_2019.csv','w',encoding='utf-8',newline='')\n",
    "wr = csv.writer(f)\n",
    "wr.writerow(['title','artist','year','duration','dominant','Positive','Negative','Anger','Anticipation','Disgust','Fear','Joy','Sadness','Surprise','Trust','Love'])\n",
    "for i in song_list:\n",
    "    wr.writerow([i['title'],i['artist'],'2019',i['duration'],i['sentiment']['dominant_emo'],i['sentiment']['Positive'],i['sentiment']['Negative'],i['sentiment']['Anger'],i['sentiment']['Anticipation'],i['sentiment']['Disgust'],i['sentiment']['Fear'],i['sentiment']['Joy'],i['sentiment']['Sadness'],i['sentiment']['Surprise'],i['sentiment']['Trust'],i['sentiment']['Love']])\n",
    "f.close()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
